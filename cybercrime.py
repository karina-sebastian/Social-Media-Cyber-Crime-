# -*- coding: utf-8 -*-
"""cybercrime.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iyvPGfKNZkroOdSSN7OP6QDdZH8jbgLr
"""

# Install required libraries
!pip install pandas nltk scikit-learn sumy # Change 'sklearn' to 'scikit-learn'

import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from sumy.summarizers.lsa import LsaSummarizer # Import the summarizer from sumy
from sumy.nlp.tokenizers import Tokenizer
from sumy.parsers.plaintext import PlaintextParser

from google.colab import files
uploaded = files.upload()

import io
df = pd.read_csv('Tweets.csv')
df.head()

# Check for missing values
print(df.isnull().sum())

# Display data types of each column
print(df.dtypes)

# Summary statistics for numerical columns
print(df.describe())

# Value counts for categorical columns
for col in data.select_dtypes(include=['object']).columns:
  print(f"\nValue counts for {col}:\n{data[col].value_counts()}")

# Example: Analyze tweet lengths
data['tweet_length'] = data['text'].astype(str).apply(len)  # Convert to string first
print(data['tweet_length'].describe())

# Example: Analyze sentiment (assuming 'sentiment' column exists)
if 'sentiment' in data.columns:
    print(data['sentiment'].value_counts())
    # You can add more detailed sentiment analysis here using libraries like TextBlob or VADER

# Define a list of keywords related to cybercrime
cybercrime_keywords = [
    "hacking", "cyber attack", "phishing", "scam", "fraud", "malware",
    "ransomware", "data breach", "identity theft","bullying", "cyberbullying"
]

# Download necessary NLTK data
nltk.download('punkt')

import nltk
nltk.download('punkt_tab')

def preprocess_text(text):
    # Check if text is a string before applying lower()
    if isinstance(text, str):
        tokens = word_tokenize(text.lower())  # Convert to lowercase and tokenize
    else:
        # Handle non-string values (e.g., convert to string or skip)
        tokens = []  # or tokens = word_tokenize(str(text))
    return tokens

df['tokens'] = df['text'].apply(preprocess_text)

df['tokens']

import nltk
nltk.download('wordnet')
nltk.download('omw-1.4')
from nltk.stem import PorterStemmer, WordNetLemmatizer

stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()

def stem_and_lemmatize(tokens):
    stemmed_tokens = [stemmer.stem(token) for token in tokens]
    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]
    return stemmed_tokens, lemmatized_tokens

df[['stemmed_tokens', 'lemmatized_tokens']] = df['tokens'].apply(lambda x: pd.Series(stem_and_lemmatize(x)))
print(df[['tokens', 'stemmed_tokens', 'lemmatized_tokens']])

# Check for cybercrime keywords and label the text
def is_cybercrime(text_tokens):
    for token in text_tokens:
        if token in cybercrime_keywords:
            return 1  # 1 = Cybercrime-related
    return 0  # 0 = Not cybercrime-related

df['cybercrime_label'] = df['tokens'].apply(is_cybercrime)

# Summarize the text using TextRank (summa library)
def generate_summary(text):
    parser = PlaintextParser.from_string(text, Tokenizer("english"))  # Create a parser object
    summarizer = LsaSummarizer()  # Create an instance of LsaSummarizer
    summary = summarizer(parser.document, 10)  # Get the summary (10 sentences)
    return " ".join([str(sentence) for sentence in summary])  # Join sentences into a string

df['summary'] = df['text'].apply(generate_summary)

# Display the processed dataset
print("Processed Dataset:")
print(df)

# Save the output to a CSV file (optional)
df.to_csv("processed_cybercrime_data.csv", index=False)

# Print a sample of the results
print("\nSample Results:")
print(df[['text', 'cybercrime_label', 'summary']])

from sumy.summarizers.text_rank import TextRankSummarizer # Import TextRankSummarizer

def generate_summary(text):
    text = str(text)  # Convert text to string to handle potential non-string values
    if len(text.split()) <= 10:  # For short texts, return the input directly
        return text
    else:  # For longer texts, use the TextRank summarizer
        summarizer = TextRankSummarizer()
        parser = PlaintextParser.from_string(text, Tokenizer("english"))
        summary = summarizer(parser.document, 10)  # Get the summary (10 sentences)
        summary_text = " ".join([str(sentence) for sentence in summary])  # Join sentences into a string
        return summary_text or text  # Fallback to original if summarization fails

df['summary'] = df['text'].apply(generate_summary)

# prompt: enter a text to check

# Assuming you want to analyze a new text input
new_text = input("Enter the text to check: ")

# Preprocess the new text
new_tokens = preprocess_text(new_text)

# Check for cybercrime keywords
new_cybercrime_label = is_cybercrime(new_tokens)

# Generate a summary for the new text
new_summary = generate_summary(new_text)

# Print the results
print("\nNew Text Analysis:")
print(f"Text: {new_text}")
print(f"Cybercrime Label: {new_cybercrime_label}")
print(f"Summary: {new_summary}")